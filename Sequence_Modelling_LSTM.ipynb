{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nIIZZvdK6-cU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset from Keras, keeping only the top 10,000 most frequent words\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "# Pad sequences to ensure uniform length for all reviews\n",
        "max_len_short = 200  # Max length for short sequences (IMDB reviews)\n",
        "X_train_padded = pad_sequences(X_train, maxlen=max_len_short)\n",
        "X_test_padded = pad_sequences(X_test, maxlen=max_len_short)\n",
        "\n",
        "print(f\"X_train_padded shape: {X_train_padded.shape}, X_test_padded shape: {X_test_padded.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cxMtUqZ7DdY",
        "outputId": "b71d0d09-f12d-4a08-fbae-ecc49f61d85c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "X_train_padded shape: (25000, 200), X_test_padded shape: (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate long sequences (e.g., Wikipedia-like data with 1500 tokens per sequence)\n",
        "long_sequences = np.random.randint(10000, size=(1000, 1500))  # 1000 samples, each with 1500 tokens\n",
        "labels_long = np.random.randint(2, size=(1000,))  # Binary labels for the simulated long sequences\n",
        "\n",
        "# Split long sequence dataset into training and testing sets\n",
        "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(long_sequences, labels_long, test_size=0.2)\n",
        "\n",
        "# Pad long sequences to a fixed length of 500 tokens\n",
        "max_len_long = 500  # Truncate/pad sequences to 500 tokens\n",
        "X_train_long_padded = pad_sequences(X_train_long, maxlen=max_len_long)\n",
        "X_test_long_padded = pad_sequences(X_test_long, maxlen=max_len_long)\n",
        "\n",
        "print(f\"X_train_long_padded shape: {X_train_long_padded.shape}, X_test_long_padded shape: {X_test_long_padded.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqWUu67x7EsI",
        "outputId": "ce24d96b-1497-4681-fe85-b359ee3839e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_long_padded shape: (800, 500), X_test_long_padded shape: (200, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build a Sequential LSTM model\n",
        "def build_model_lstm(input_length, units=64, num_layers=1):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_length))\n",
        "    for i in range(num_layers):\n",
        "        if i == num_layers - 1:\n",
        "            model.add(LSTM(units, return_sequences=False))\n",
        "        else:\n",
        "            model.add(LSTM(units, return_sequences=True))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "kHNMHZJj7HKT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate the model\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, units=64, num_layers=1, max_len=200):\n",
        "    model = build_model_lstm(input_length=max_len, units=units, num_layers=num_layers)\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.round(y_pred).flatten()  # Convert predictions to binary labels\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"LSTM - Units: {units}, Layers: {num_layers}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "gNjNm3jq7LLd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTMs on the IMDB dataset (short sequences)\n",
        "print(\"Short Sequence Dataset (IMDB)\")\n",
        "short_seq_results = []\n",
        "\n",
        "# LSTM with 1 layer, 64 units\n",
        "print(\"Evaluating LSTM (1 layer, 64 units)\")\n",
        "short_seq_results.append(train_and_evaluate(X_train_padded, X_test_padded, y_train, y_test, units=64, num_layers=1, max_len=max_len_short))\n",
        "\n",
        "# LSTM with 2 layers, 64 units\n",
        "print(\"Evaluating LSTM (2 layers, 64 units)\")\n",
        "short_seq_results.append(train_and_evaluate(X_train_padded, X_test_padded, y_train, y_test, units=64, num_layers=2, max_len=max_len_short))\n",
        "\n",
        "# LSTM with 1 layer, 128 units\n",
        "print(\"Evaluating LSTM (1 layer, 128 units)\")\n",
        "short_seq_results.append(train_and_evaluate(X_train_padded, X_test_padded, y_train, y_test, units=128, num_layers=1, max_len=max_len_short))\n",
        "\n",
        "# LSTM with 2 layers, 128 units\n",
        "print(\"Evaluating LSTM (2 layers, 128 units)\")\n",
        "short_seq_results.append(train_and_evaluate(X_train_padded, X_test_padded, y_train, y_test, units=128, num_layers=2, max_len=max_len_short))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQBO_HX57ME1",
        "outputId": "125f4e62-1d3a-4946-9bda-614cad883ac6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short Sequence Dataset (IMDB)\n",
            "Evaluating LSTM (1 layer, 64 units)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 315ms/step - accuracy: 0.7068 - loss: 0.5315 - val_accuracy: 0.8454 - val_loss: 0.3528\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 331ms/step - accuracy: 0.9036 - loss: 0.2517 - val_accuracy: 0.8742 - val_loss: 0.3342\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 295ms/step - accuracy: 0.9173 - loss: 0.2079 - val_accuracy: 0.8534 - val_loss: 0.3590\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 320ms/step - accuracy: 0.9574 - loss: 0.1240 - val_accuracy: 0.8616 - val_loss: 0.4416\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 297ms/step - accuracy: 0.9620 - loss: 0.1058 - val_accuracy: 0.7820 - val_loss: 0.5167\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step\n",
            "LSTM - Units: 64, Layers: 1, Accuracy: 0.7901\n",
            "Evaluating LSTM (2 layers, 64 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 464ms/step - accuracy: 0.6989 - loss: 0.5402 - val_accuracy: 0.8734 - val_loss: 0.3144\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 447ms/step - accuracy: 0.9045 - loss: 0.2507 - val_accuracy: 0.8702 - val_loss: 0.3104\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 452ms/step - accuracy: 0.9445 - loss: 0.1571 - val_accuracy: 0.8550 - val_loss: 0.3512\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 445ms/step - accuracy: 0.9593 - loss: 0.1177 - val_accuracy: 0.8602 - val_loss: 0.3813\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 454ms/step - accuracy: 0.9743 - loss: 0.0778 - val_accuracy: 0.8380 - val_loss: 0.5227\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step\n",
            "LSTM - Units: 64, Layers: 2, Accuracy: 0.8455\n",
            "Evaluating LSTM (1 layer, 128 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 549ms/step - accuracy: 0.7121 - loss: 0.5368 - val_accuracy: 0.8610 - val_loss: 0.3381\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 545ms/step - accuracy: 0.8976 - loss: 0.2623 - val_accuracy: 0.8652 - val_loss: 0.3141\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 526ms/step - accuracy: 0.9295 - loss: 0.1891 - val_accuracy: 0.8644 - val_loss: 0.3313\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 528ms/step - accuracy: 0.9534 - loss: 0.1384 - val_accuracy: 0.8448 - val_loss: 0.3746\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 527ms/step - accuracy: 0.9671 - loss: 0.0947 - val_accuracy: 0.8642 - val_loss: 0.4308\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step\n",
            "LSTM - Units: 128, Layers: 1, Accuracy: 0.8511\n",
            "Evaluating LSTM (2 layers, 128 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 1s/step - accuracy: 0.7233 - loss: 0.5221 - val_accuracy: 0.8586 - val_loss: 0.3450\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.8931 - loss: 0.2780 - val_accuracy: 0.8714 - val_loss: 0.3177\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 1s/step - accuracy: 0.9348 - loss: 0.1770 - val_accuracy: 0.8606 - val_loss: 0.3518\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9575 - loss: 0.1184 - val_accuracy: 0.8638 - val_loss: 0.4169\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 1s/step - accuracy: 0.9749 - loss: 0.0745 - val_accuracy: 0.8524 - val_loss: 0.4672\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 228ms/step\n",
            "LSTM - Units: 128, Layers: 2, Accuracy: 0.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTMs on the simulated long sequences (e.g., Wikipedia-like)\n",
        "print(\"\\nLong Sequence Dataset (Simulated Wikipedia)\")\n",
        "long_seq_results = []\n",
        "\n",
        "# LSTM with 1 layer, 64 units\n",
        "print(\"Evaluating LSTM (1 layer, 64 units)\")\n",
        "long_seq_results.append(train_and_evaluate(X_train_long_padded, X_test_long_padded, y_train_long, y_test_long, units=64, num_layers=1, max_len=max_len_long))\n",
        "\n",
        "# LSTM with 2 layers, 64 units\n",
        "print(\"Evaluating LSTM (2 layers, 64 units)\")\n",
        "long_seq_results.append(train_and_evaluate(X_train_long_padded, X_test_long_padded, y_train_long, y_test_long, units=64, num_layers=2, max_len=max_len_long))\n",
        "\n",
        "# LSTM with 1 layer, 128 units\n",
        "print(\"Evaluating LSTM (1 layer, 128 units)\")\n",
        "long_seq_results.append(train_and_evaluate(X_train_long_padded, X_test_long_padded, y_train_long, y_test_long, units=128, num_layers=1, max_len=max_len_long))\n",
        "\n",
        "# LSTM with 2 layers, 128 units\n",
        "print(\"Evaluating LSTM (2 layers, 128 units)\")\n",
        "long_seq_results.append(train_and_evaluate(X_train_long_padded, X_test_long_padded, y_train_long, y_test_long, units=128, num_layers=2, max_len=max_len_long))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hclh_sqq7P79",
        "outputId": "36991820-092e-439e-adb1-f4f4e4d6a863"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Long Sequence Dataset (Simulated Wikipedia)\n",
            "Evaluating LSTM (1 layer, 64 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 782ms/step - accuracy: 0.4790 - loss: 0.6938 - val_accuracy: 0.4875 - val_loss: 0.6940\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 648ms/step - accuracy: 0.8301 - loss: 0.6773 - val_accuracy: 0.4938 - val_loss: 0.6956\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 852ms/step - accuracy: 0.9841 - loss: 0.5962 - val_accuracy: 0.5063 - val_loss: 0.7077\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 616ms/step - accuracy: 1.0000 - loss: 0.3516 - val_accuracy: 0.4938 - val_loss: 0.8424\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 724ms/step - accuracy: 0.9997 - loss: 0.0822 - val_accuracy: 0.4812 - val_loss: 1.0503\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step\n",
            "LSTM - Units: 64, Layers: 1, Accuracy: 0.4900\n",
            "Evaluating LSTM (2 layers, 64 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 950ms/step - accuracy: 0.5042 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6939\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5652 - loss: 0.6768 - val_accuracy: 0.4938 - val_loss: 0.6979\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.9827 - loss: 0.4669 - val_accuracy: 0.4938 - val_loss: 0.8542\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 911ms/step - accuracy: 1.0000 - loss: 0.0662 - val_accuracy: 0.5250 - val_loss: 1.5425\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 940ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.4750 - val_loss: 2.2870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 790 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dc43a96ea70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step\n",
            "LSTM - Units: 64, Layers: 2, Accuracy: 0.5050\n",
            "Evaluating LSTM (1 layer, 128 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.4966 - loss: 0.6934 - val_accuracy: 0.5063 - val_loss: 0.6933\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.7466 - loss: 0.6750 - val_accuracy: 0.4688 - val_loss: 0.6997\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.9567 - loss: 0.5274 - val_accuracy: 0.5063 - val_loss: 0.7306\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.9803 - loss: 0.1958 - val_accuracy: 0.5312 - val_loss: 0.8746\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.9957 - loss: 0.0429 - val_accuracy: 0.5500 - val_loss: 1.0685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dc43db0f910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 304ms/step\n",
            "LSTM - Units: 128, Layers: 1, Accuracy: 0.5000\n",
            "Evaluating LSTM (2 layers, 128 units)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4s/step - accuracy: 0.5368 - loss: 0.6937 - val_accuracy: 0.5437 - val_loss: 0.6932\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.6239 - loss: 0.6744 - val_accuracy: 0.5000 - val_loss: 0.7499\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.9467 - loss: 0.3443 - val_accuracy: 0.4500 - val_loss: 0.8226\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.9848 - loss: 0.1122 - val_accuracy: 0.5000 - val_loss: 1.7453\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3s/step - accuracy: 0.9981 - loss: 0.0094 - val_accuracy: 0.5125 - val_loss: 1.9499\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 494ms/step\n",
            "LSTM - Units: 128, Layers: 2, Accuracy: 0.5000\n"
          ]
        }
      ]
    }
  ]
}